import json, os, time, platform
import pandas as pd
from datetime import datetime, timezone

REFRESH_INTERVAL = 5 # secondes
TABLE_WIDTH = 70

try:
    from config import AUDIT_FILE, PARQUET_PATH, AUDIT_BATCH_SIZE
except ImportError:
    print("‚ùå ERREUR : Impossible de trouver src/config.py")
    print("Assurez-vous de lancer le script depuis la racine du projet !")
    exit(1)

# V√©rification WSL 
if platform.system() == "Windows":
    print("‚ö†Ô∏è  ERREUR : Ce script est con√ßu pour WSL/Linux.")
    

def get_producer_count():
    """Lire le fichier t√©moin g√©n√©r√© par le producer"""
    if not os.path.exists(AUDIT_FILE):
        return 0, "Non d√©marr√©"
    
    try:
        with open(AUDIT_FILE, 'r') as f:
            data = json.load(f)
            # On r√©cup√®re le nombre de messages confirm√©s dans 'metrics'
            return data["metrics"]["confirmed"], data["last_update"]
    except Exception as e:
        return 0, f"Erreur lecture: {e}"

def show_stats():
    # Nettoyage console
    os.system('clear')
    
    # 1. R√âCUP√âRATION DES CHIFFRES
    prod_count, prod_last_seen = get_producer_count()
    
    last_update_str = prod_last_seen
    if "T" in str(prod_last_seen):
        try:
            # On coupe pour garder HH:MM:SS
            last_update_str = prod_last_seen.split('T')[1].split('.')[0]
        except:
            pass # Si format bizarre, on laisse tel quel
    
    spark_count = 0
    df = pd.DataFrame()
    spark_status = "‚è≥ En attente d'initialisation..."
    
    try:
        # Lecture optimis√©e avec PyArrow engine
        if os.path.exists(PARQUET_PATH):
            df = pd.read_parquet(PARQUET_PATH, engine='pyarrow')
            spark_count = len(df)
            spark_status = "üü¢ Disponible"
        else:
            spark_status = "‚ö†Ô∏è Dossier introuvable"
    except Exception as e:
        spark_status = f"‚ùå Erreur: {e}"

    # 2. CALCUL DU LAG (Diff√©rence)
    diff = prod_count - spark_count
    
    # --- AFFICHAGE DASHBOARD ---
    print(f"üìä MONITORING PIPELINE E2E | {datetime.now(timezone.utc).strftime('%H:%M:%S')} | Rafra√Æchissement: {REFRESH_INTERVAL}s")
    print("="*TABLE_WIDTH)
    
    print(f"üó£Ô∏è  PRODUCER (Source)   :   {str(prod_count).ljust(6)} tickets confirm√©s   [MAJ: {last_update_str}]")    
    print(f"üíæ  DATALAKE (Cible)    :   {str(spark_count).ljust(6)} tickets stock√©s")
    print(f"üìÇ  ACC√àS DATALAKE      :   [{spark_status}]")
  
    print("-" * TABLE_WIDTH)
    
    if diff == 0 and prod_count > 0:
        print(f"‚úÖ SYNCHRONISATION PARFAITE (Zero Data Loss)")
    elif diff > 0:
        print(f"‚ö†Ô∏è  LAG D√âTECT√â : {diff} tickets en cours de traitement...")
        print("    (Ils sont dans Redpanda ou dans le buffer Spark)")
    elif diff < 0:        
        gap = abs(diff)        
        # Cas "Normal" : L'√©cart est petit (inf√©rieur √† 2 batchs du producer)
        # Cela veut dire que Spark a bien lu les tickets, mais le Producer n'a pas encore √©crit dans le JSON.
        if gap <= AUDIT_BATCH_SIZE * 2:
             print(f"üöÄ AVANCE SPARK : {gap} tickets d'avance sur le fichier t√©moin.")
             print("    (Normal : Le fichier d'audit Producer ne se met √† jour que tous les 10 tickets)")
        
        # Cas "Anormal" : L'√©cart est √©norme
        # L√†, c'est s√ªrement qu'on a oubli√© de vider le dossier avant de commencer
        else:
             print(f"üëª DONN√âES FANT√îMES : {gap} tickets inattendus dans le Data Lake.")
             print("    (Cause probable : Oubli de nettoyage avant le lancement)")
    else:
        print("üí§ En attente de donn√©es...")

    print("="*TABLE_WIDTH)
    
# Boucle infinie
try:
    while True:
        show_stats()
        time.sleep(REFRESH_INTERVAL)
except KeyboardInterrupt:
    print("\n Arr√™t du monitoring.")